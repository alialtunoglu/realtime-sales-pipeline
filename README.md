# ğŸ›ï¸ Real-time Sales Pipeline

Modern **Medallion Architecture** ile geliÅŸtirilmiÅŸ retail satÄ±ÅŸ veri pipeline'Ä±. Apache Spark, Delta Lake, ve Apache Airflow kullanarak Bronze-Silver-Gold katmanlÄ± veri iÅŸleme sistemi.

## ğŸ—ï¸ Mimari

```
ğŸ“ CSV Data â†’ ğŸ¥‰ Bronze â†’ ğŸ¥ˆ Silver â†’ ğŸ¥‡ Gold â†’ ğŸ”¬ Advanced Analytics â†’ ğŸ“Š Dashboard
```

### Katmanlar:
- **ğŸ¥‰ Bronze:** Ham veri (Delta format)
- **ğŸ¥ˆ Silver:** TemizlenmiÅŸ veri (Validated)  
- **ğŸ¥‡ Gold:** Ä°ÅŸ analitiÄŸi tablolarÄ±
- **ğŸ”¬ Advanced:** RFM, CLTV, Forecasting

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1ï¸âƒ£ Ortam HazÄ±rlÄ±ÄŸÄ±
```bash
# Conda environment aktif et
conda activate spark-delta-env

# Gerekli paketleri kur (eÄŸer yoksa)
pip install pyspark delta-spark apache-airflow streamlit plotly
```

### 2ï¸âƒ£ Pipeline'Ä± Ã‡alÄ±ÅŸtÄ±r
```bash
# HÄ±zlÄ± test
./test_pipeline.sh

# Tam pipeline
./run_complete_pipeline.sh
```

### 3ï¸âƒ£ Dashboard'u BaÅŸlat
```bash
streamlit run dashboard/app.py --server.port 8504
```

## ğŸ¤– Airflow Otomasyonu

### DAG'larÄ± Ã‡alÄ±ÅŸtÄ±r
```bash
# Airflow baÅŸlat
export AIRFLOW_HOME=$(pwd)/airflow
airflow webserver --port 8080 &
airflow scheduler &

# Pipeline tetikle
airflow dags trigger retail_sales_pipeline
```

### KullanÄ±labilir DAG'lar:
- `retail_sales_pipeline` - Tam pipeline
- `bronze_ingestion_dag` - Sadece Bronze layer
- `silver_cleaning_dag` - Sadece Silver layer

## ğŸ“Š OluÅŸturulan Tablolar

### Gold Layer:
- `daily_sales` - GÃ¼nlÃ¼k satÄ±ÅŸ Ã¶zetleri
- `top_products` - En Ã§ok satÄ±lan Ã¼rÃ¼nler  
- `country_sales` - Ãœlke bazlÄ± satÄ±ÅŸ

### Advanced Analytics:
- `rfm_table` - MÃ¼ÅŸteri segmentasyonu
- `cltv_table` - MÃ¼ÅŸteri yaÅŸam boyu deÄŸeri
- `forecast_table` - 12 aylÄ±k gelir tahminleri

## ğŸ“ Proje YapÄ±sÄ±

```
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                    # Airflow DAG dosyalarÄ±
â”‚   â””â”€â”€ airflow.cfg             # Airflow konfigÃ¼rasyonu
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                  # Streamlit dashboard
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                  # Ham veri dosyalarÄ±
â”‚   â””â”€â”€ output/                 # Ä°ÅŸlenmiÅŸ veriler
â”œâ”€â”€ delta/                      # Delta Lake tablolarÄ±
â”‚   â”œâ”€â”€ bronze/                 # Ham veriler
â”‚   â”œâ”€â”€ silver/                 # TemizlenmiÅŸ veriler
â”‚   â””â”€â”€ gold/                   # Ä°ÅŸ analitiÄŸi
â”œâ”€â”€ notebooks/                  # Jupyter Notebooks
â”‚   â”œâ”€â”€ Complete_Pipeline_Tutorial.ipynb
â”‚   â”œâ”€â”€ 01_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_silver_cleaning.ipynb
â”‚   â”œâ”€â”€ 03_gold_aggregation.ipynb
â”‚   â”œâ”€â”€ gold_rfm.ipynb
â”‚   â”œâ”€â”€ gold_cltv.ipynb
â”‚   â””â”€â”€ gold_forecasting.ipynb
â”œâ”€â”€ tasks/                      # Pipeline task'leri
â”‚   â”œâ”€â”€ ingest_data.py          # Bronze layer
â”‚   â”œâ”€â”€ clean_data.py           # Silver layer
â”‚   â”œâ”€â”€ generate_gold.py        # Gold layer
â”‚   â””â”€â”€ advanced_analytics.py   # RFM, CLTV, Forecasting
â”œâ”€â”€ run_complete_pipeline.sh    # Tam pipeline script
â”œâ”€â”€ test_pipeline.sh           # HÄ±zlÄ± test script
â””â”€â”€ requirements.txt           # Python dependencies
```

## ğŸ› ï¸ Teknoloji Stack

- **Veri Ä°ÅŸleme:** Apache Spark + Delta Lake
- **Orchestration:** Apache Airflow
- **Visualization:** Streamlit + Plotly
- **Analytics:** PySpark SQL + MLlib
- **Storage:** Delta Lake (Parquet + Transaction Log)

## ğŸ“ˆ Analytics Ã–zellikleri

### RFM Analizi
- **Recency:** Son alÄ±ÅŸveriÅŸten bu yana geÃ§en gÃ¼n
- **Frequency:** Toplam alÄ±ÅŸveriÅŸ sayÄ±sÄ±  
- **Monetary:** Toplam harcama miktarÄ±

### CLTV (Customer Lifetime Value)
```
CLTV = (Frequency Ã— Monetary) / (Recency + 1)
```

### Forecasting
```
12 AylÄ±k Tahmini Gelir = CLTV Ã— 12
```

## ğŸ”§ KonfigÃ¼rasyon

### Spark AyarlarÄ±
```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
```

### Delta Lake AyarlarÄ±
```python
spark.conf.set("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
spark.conf.set("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
```

## ğŸ“š Ã–ÄŸrenme KaynaklarÄ±

1. **ğŸ““ Complete_Pipeline_Tutorial.ipynb** - Tam tutorial
2. **ğŸ“Š Dashboard** - http://localhost:8504
3. **ğŸ¤– Airflow UI** - http://localhost:8080

## ğŸ› Sorun Giderme

### Streamlit TypeIs HatasÄ±
```bash
pip install --upgrade typing-extensions
```

### Spark Port Ã‡akÄ±ÅŸmasÄ±
```bash
# FarklÄ± port kullan
spark.conf.set("spark.ui.port", "4041")
```

### Delta Table Okuma HatasÄ±
```bash
# Cache temizle
spark.sql("CLEAR CACHE")
```

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun (`git checkout -b feature/amazing-feature`)
3. Commit yapÄ±n (`git commit -m 'Add amazing feature'`)
4. Push yapÄ±n (`git push origin feature/amazing-feature`)
5. Pull Request aÃ§Ä±n

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

## ğŸ‘¨â€ğŸ’» GeliÅŸtirici

Medallion Architecture ve modern veri mÃ¼hendisliÄŸi prensipleri ile geliÅŸtirilmiÅŸtir.

---

**ğŸ¯ Bu proje ile neler Ã¶ÄŸreneceksiniz:**
- Modern veri mÃ¼hendisliÄŸi best practice'leri
- Apache Spark ile bÃ¼yÃ¼k veri iÅŸleme
- Delta Lake ile ACID transaction'lar
- Apache Airflow ile pipeline otomasyonu
- Streamlit ile real-time dashboard'lar
- RFM, CLTV ve forecasting analitiÄŸi
